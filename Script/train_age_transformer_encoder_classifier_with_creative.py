import os
import sys
import numpy as np 
import pandas as pd
import logging
import gc
import tqdm
import pickle
import json
import time
import tempfile
from gensim.models import Word2Vec
from sklearn.metrics import accuracy_score, roc_auc_score

import torch
from torch import nn
import torch.nn.functional as F

from data_loader import train_data_loader, test_data_loader
from transformer_encoder_classifier import Transformer_Encoder_Classifier

cwd = os.getcwd()
train_path = os.path.join(cwd, 'train_artifact')
test_path = os.path.join(cwd, 'test_artifact')
input_path = os.path.join(cwd, 'input_artifact')
input_split_path = os.path.join(cwd, 'input_split_artifact')
embed_path = os.path.join(cwd, 'embed_artifact')
model_path = os.path.join(cwd, 'model_artifact')

registry_path = os.path.join(embed_path, 'w2v_registry.json')
with open(registry_path, 'r') as f:
	w2v_registry = json.load(f)

def initiate_logger(log_path):
	"""
	Initialize a logger with file handler and stream handler
	"""
	logger = logging.getLogger(__name__)
	logger.setLevel(logging.INFO)
	formatter = logging.Formatter('%(asctime)s %(levelname)-s: %(message)s', datefmt='%H:%M:%S')
	fh = logging.FileHandler(log_path)
	fh.setLevel(logging.INFO)
	fh.setFormatter(formatter)
	logger.addHandler(fh)
	sh = logging.StreamHandler(sys.stdout)
	sh.setLevel(logging.INFO)
	sh.setFormatter(formatter)
	logger.addHandler(sh)
	logger.info('===================================')
	logger.info('Begin executing at {}'.format(time.ctime()))
	logger.info('===================================')
	return logger

def get_torch_module_num_of_parameter(model):
	"""
	Get # of parameters in a torch module.
	"""
	model_parameters = filter(lambda p: p.requires_grad, model.parameters())
	params = sum([np.prod(p.size()) for p in model_parameters])
	return params

def train(model, train_inp_tuple, validation_inp_tuple, checkpoint_dir, checkpoint_prefix, device, epoches=5, batch_size=1024, logger=None, epoch_start=0, max_seq_len=100, lr=1e-3):
	"""
	: model (torch.nn.module): model to be trained
	: train_inp_tuple (list[tuple(str, list[str], list[str])]): list of input for train_data_loader
		: str: path to label data
		: list[str]: list of embedding variables
		: list[str]: list of paths to a pkl file 
	: validation_inp_tuple (list[tuple(str, list[str], list[str])]): list of input for train_data_loader
		: str: path to label data
		: list[str]: list of embedding variables
		: list[str]: list of paths to a pkl file
	: checkpoint_dir (str): path to checkpoint directory
	: checkpoint_prefix (str): prefix of checkpoint file
	: device (str): device to train the model
	: epoches (int): number of epoches to train
	: batch_size (int): size of mini batch
	: epoch_start (int): if = 0 then train a new model, else load an existing model and continue to train, default 0
	: max_seq_len (int): max length for sequence input, default 100 
	: lr (float): learning rate for Adam, default 1e-3
	"""
	global w2v_registry, model_path
	gc.enable()

	# Check checkpoint directory
	if not os.path.isdir(checkpoint_dir): os.mkdir(checkpoint_dir)

	# Load model if not train from scratch
	if epoch_start != 0:
		model_artifact_path = os.path.join(checkpoint_dir, '{}_{}.pth'.format(checkpoint_prefix, epoch_start))
		model.load_state_dict(torch.load(model_artifact_path))
		if logger: logger.info('Start retraining from epoch {}'.format(epoch_start))
	print("model_artifact_path=", model_artifact_path)
	# Set up loss function and optimizer
	model.to(device)
	loss_fn = nn.CrossEntropyLoss()
	optimizer = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=True)

	div, mod = divmod(810000, batch_size)
	n_batch_estimate = div + min(mod, 1)

	# Main Loop
	for epoch in range(1+epoch_start, epoches+1+epoch_start):
		if logger:
			logger.info('=========================')
			logger.info('Processing Epoch {}/{}'.format(epoch, epoches+epoch_start))
			logger.info('=========================')

		# Train model
		model.train()
		train_running_loss, train_n_batch = 0, 0

		for index, (label_artifact_path, seq_inp_target, seq_inp_path) in enumerate(train_inp_tuple, start=1):
			print("epoch=", epoch, "index=", index, "label_artifac_path=", label_artifact_path,
				  "seq_inp_target=", seq_inp_target, "seq_inp_path=", seq_inp_path)
			train_loader = train_data_loader(label_artifact_path, seq_inp_target, seq_inp_path, w2v_registry, batch_size=batch_size, max_seq_len=max_seq_len)
			train_iterator = iter(train_loader)
			while True:
				try:
					y, x_seq, x_last_idx = next(train_iterator)  # 6输入，6个序列  [6, batch_size, max_seq_len, embed_size]
					y = torch.from_numpy(y).long().to(device)
					x = []
					for s in x_seq:
						x.append(s.to(device))
					x.append(x_last_idx)  # x中加入序列长度-1
					optimizer.zero_grad()
					yp = F.softmax(model(*x), dim=1)
					loss = loss_fn(yp, y)

					loss.backward()
					torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)
					optimizer.step()

					train_running_loss += loss.item()
					train_n_batch += 1

					if train_n_batch%100==0 and logger:
						logger.info('Epoch {}/{} - Batch {}/{} Done - Train Loss: {:.6f}'.format(epoch, epoches+epoch_start, train_n_batch, n_batch_estimate, train_running_loss/train_n_batch))
					del x, y, yp, x_seq, x_last_idx
					_ = gc.collect()
					torch.cuda.empty_cache()

				except StopIteration:
					break

			del train_loader, train_iterator
			_ = gc.collect()
			torch.cuda.empty_cache()

			if logger:
				logger.info('Epoch {}/{} - Batch {}/{} Done - Train Loss: {:.6f}'.format(epoch, epoches+epoch_start, train_n_batch, n_batch_estimate, train_running_loss/train_n_batch))

		# Evaluate model
		model.eval()
		test_running_loss, test_n_batch = 0, 0
		true_y, pred_y = [], []

		for index, (label_artifact_path, seq_inp_target, seq_inp_path) in enumerate(validation_inp_tuple, start=1):
			train_loader = train_data_loader(label_artifact_path, seq_inp_target, seq_inp_path, w2v_registry, batch_size=batch_size, max_seq_len=max_seq_len)
			train_iterator = iter(train_loader)
			while True:
				try:
					y, x_seq, x_last_idx = next(train_iterator)
					y = torch.from_numpy(y).long().to(device)
					x = []
					for s in x_seq:
						x.append(s.to(device))
					x.append(x_last_idx)
					yp = F.softmax(model(*x), dim=1)
					loss = loss_fn(yp, y)

					pred_y.extend(list(yp.cpu().detach().numpy()))
					true_y.extend(list(y.cpu().detach().numpy()))

					test_running_loss += loss.item()
					test_n_batch += 1

					del x, y, yp, x_seq, x_last_idx
					_ = gc.collect()
					torch.cuda.empty_cache()

				except StopIteration:
					break

			del train_loader, train_iterator
			_ = gc.collect()
			torch.cuda.empty_cache()

		pred = np.argmax(np.array(pred_y), 1)
		true = np.array(true_y).reshape((-1,))
		acc_score = accuracy_score(true, pred)

		del pred, true, pred_y, true_y
		_ = gc.collect()
		torch.cuda.empty_cache()

		if logger:
			logger.info('Epoch {}/{} Done - Test Loss: {:.6f}, Test Accuracy: {:.6f}'.format(epoch, epoches+epoch_start, test_running_loss/test_n_batch, acc_score))

		# Save model state dict
		ck_file_name = '{}_{}.pth'.format(checkpoint_prefix, epoch)
		ck_file_path = os.path.join(checkpoint_dir, ck_file_name)
		
		torch.save(model.state_dict(), ck_file_path)

if __name__=='__main__':
	assert len(sys.argv)>=6
	epoch_start = int(sys.argv[1])
	epoches = int(sys.argv[2])
	batch_size = int(sys.argv[3])
	max_seq_len = int(sys.argv[4])
	lr = float(sys.argv[5])
	print("get params=", sys.argv)
	if len(sys.argv)>6:
		train_inp_tuple = [(os.path.join(input_split_path, 'train_age_{}.npy'.format(i)), ['creative'], 
			[os.path.join(input_split_path, 'train_creative_id_seq_{}.pkl'.format(i))]) for i in range(1,10)]
		validation_inp_tuple = [(os.path.join(input_split_path, 'train_age_{}.npy'.format(i)), ['creative'], 
			[os.path.join(input_split_path, 'train_creative_id_seq_{}.pkl'.format(i))]) for i in range(10,11)]
		checkpoint_dir = os.path.join(model_path, 'Transformer_Encoder_Classifier_Creative_Age')
		checkpoint_prefix = 'Transformer_Encoder_Classifier_Creative_Age'
	else:
		train_inp_tuple = [(os.path.join(input_path, 'train_age_tra.npy'), ['product', 'advertiser'],  # , 'creative', 'ad'
			[os.path.join(input_path, 'train_creative_id_seq_tra.pkl')])]
		validation_inp_tuple = [(os.path.join(input_path, 'train_age_val.npy'), ['product', 'advertiser'],  # , 'creative', 'ad'
			[os.path.join(input_path, 'train_creative_id_seq_val.pkl')])]
		checkpoint_dir = os.path.join(model_path, 'Transformer_Encoder_Classifier_Creative_Age')
		checkpoint_prefix = 'Transformer_Encoder_Classifier_Creative_Age'
	print("get train_inp_tuple=", train_inp_tuple, "\n\nvalidation_inp_tupe=", validation_inp_tuple, "\n\ncheckpoint_dir=", checkpoint_dir,
		  "\n\ncheckpoint_prefix=", checkpoint_prefix)

	logger = initiate_logger('Transformer_Encoder_Classifier_Creative_Age.log')
	logger.info('Epoch Start: {}， Epoch to Train: {}, Batch Size: {}, Max Sequence Length: {}, Learning Rate: {}'.format(epoch_start, epoches, batch_size, max_seq_len, lr))
	DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
	logger.info('Device in Use: {}'.format(DEVICE))
	if torch.cuda.is_available():
		torch.cuda.empty_cache()
		t = torch.cuda.get_device_properties(DEVICE).total_memory/1024**3
		c = torch.cuda.memory_cached(DEVICE)/1024**3
		a = torch.cuda.memory_allocated(DEVICE)/1024**3
		logger.info('CUDA Memory: Total {:.2f} GB, Cached {:.2f} GB, Allocated {:.2f} GB'.format(t,c,a))

	model = Transformer_Encoder_Classifier(128, 10, 4, 8, 1024, DEVICE).to(DEVICE)

	logger.info('Model Parameter #: {}'.format(get_torch_module_num_of_parameter(model)))
	
	train(model, train_inp_tuple, validation_inp_tuple, checkpoint_dir, checkpoint_prefix, DEVICE, 
		epoches=epoches, batch_size=batch_size, logger=logger, epoch_start=epoch_start, max_seq_len=max_seq_len, lr=lr)